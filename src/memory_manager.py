from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from typing import List, Dict, Any
import json
from .config import llm

class SmartMemoryManager:
    """æ™ºèƒ½è®°å¿†ç®¡ç†å™¨ - æ”¯æŒçª—å£è®°å¿†å’Œé•¿æœŸæ¨¡å¼è¿½è¸ª"""
    
    def __init__(self, max_conversation_window: int = 10, summary_trigger_ratio: float = 0.8):
        """
        åˆå§‹åŒ–æ™ºèƒ½è®°å¿†ç®¡ç†å™¨
        
        Args:
            max_conversation_window: æœ€å¤§å¯¹è¯çª—å£å¤§å°ï¼ˆä¿ç•™æœ€è¿‘Nè½®å¯¹è¯ï¼‰
            summary_trigger_ratio: é¢„ç•™å‚æ•°ï¼Œç”¨äºæœªæ¥æ‰©å±•
        """
        self.max_conversation_window = max_conversation_window
        self.conversation_count = 0
        
        # ä½¿ç”¨ConversationBufferWindowMemoryè¿›è¡Œçª—å£è®°å¿†
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=max_conversation_window,  # ä¿ç•™æœ€è¿‘kè½®å¯¹è¯
            ai_prefix="æ‹½å§",
            human_prefix="ç”¨æˆ·",
            output_key="output"  # æ˜ç¡®æŒ‡å®šè¾“å‡ºé”®ï¼Œæ¶ˆé™¤è­¦å‘Š
        )
        
        # é•¿æœŸè®°å¿†å­˜å‚¨ï¼ˆå…³é”®ä¿¡æ¯æŒä¹…åŒ–ï¼‰
        self.long_term_memory = {
            "user_patterns": {},  # ç”¨æˆ·æ‹çˆ±æ¨¡å¼è®°å½•
            "risk_history": [],   # é£é™©ç­‰çº§å†å²
            "key_insights": [],   # å…³é”®æ´å¯Ÿè®°å½•
            "persona_preferences": {}  # æµ·ç‹æ¨¡æ‹Ÿåå¥½
        }

    def add_interaction(self, user_input: str, ai_response: str, 
                       love_brain_level: str = None, risk_signals: List[str] = None):
        """æ·»åŠ ä¸€è½®å¯¹è¯åˆ°è®°å¿†ä¸­"""
        self.conversation_count += 1
        
        # æ·»åŠ åˆ°çŸ­æœŸè®°å¿†
        self.memory.save_context(
            {"input": user_input},
            {"output": ai_response}
        )
        
        # æ›´æ–°é•¿æœŸè®°å¿†ä¸­çš„å…³é”®ä¿¡æ¯
        self._update_long_term_memory(user_input, ai_response, love_brain_level, risk_signals)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©é•¿æœŸè®°å¿†
        self._maintain_long_term_memory()

    def _maintain_long_term_memory(self):
        """ç»´æŠ¤é•¿æœŸè®°å¿†ï¼Œé˜²æ­¢è¿‡åº¦è†¨èƒ€"""
        # é£é™©å†å²ä¿æŒæœ€è¿‘20æ¡
        if len(self.long_term_memory["risk_history"]) > 20:
            self.long_term_memory["risk_history"] = self.long_term_memory["risk_history"][-20:]
        
        # å…³é”®æ´å¯Ÿä¿æŒæœ€è¿‘10æ¡
        if len(self.long_term_memory["key_insights"]) > 10:
            self.long_term_memory["key_insights"] = self.long_term_memory["key_insights"][-10:]

    def _update_long_term_memory(self, user_input: str, ai_response: str, 
                                love_brain_level: str = None, risk_signals: List[str] = None):
        """æ›´æ–°é•¿æœŸè®°å¿†ä¸­çš„å…³é”®ä¿¡æ¯"""
        
        # è®°å½•é£é™©ç­‰çº§å†å²
        if love_brain_level:
            self.long_term_memory["risk_history"].append({
                "round": self.conversation_count,
                "level": love_brain_level,
                "signals": risk_signals or []
            })

        # æ£€æµ‹ç”¨æˆ·æ‹çˆ±æ¨¡å¼
        self._detect_user_patterns(user_input, love_brain_level)
        
        # æå–å…³é”®æ´å¯Ÿ
        if love_brain_level in ["é‡", "å±"]:
            insight = f"ç¬¬{self.conversation_count}è½®ï¼š{love_brain_level}çº§é£é™© - {user_input[:50]}..."
            self.long_term_memory["key_insights"].append(insight)

    def _detect_user_patterns(self, user_input: str, love_brain_level: str):
        """æ£€æµ‹ç”¨æˆ·æ‹çˆ±è¡Œä¸ºæ¨¡å¼"""
        patterns = self.long_term_memory["user_patterns"]
        
        # å…³é”®è¯æ£€æµ‹
        pattern_keywords = {
            "é‡‘é’±ä¾èµ–": ["è½¬è´¦", "å€Ÿé’±", "æŠ•èµ„", "ä¹°å•", "èŠ±é’±"],
            "æƒ…ç»ªä¾èµ–": ["æƒ³å¿µ", "ç„¦è™‘", "å¤±çœ ", "å¿ƒæƒ…", "æƒ…ç»ª"],
            "ç¤¾äº¤éš”ç¦»": ["æœ‹å‹", "å®¶äºº", "åŒäº‹", "ç¤¾äº¤", "è”ç³»"],
            "æ—¶é—´æ²‰è¿·": ["æ•´å¤©", "ä¸€ç›´", "24å°æ—¶", "ä¸åœ", "æ€»æ˜¯"]
        }
        
        for pattern_type, keywords in pattern_keywords.items():
            if any(keyword in user_input for keyword in keywords):
                if pattern_type not in patterns:
                    patterns[pattern_type] = 0
                patterns[pattern_type] += 1

    def _check_and_optimize_memory(self):
        """æ£€æŸ¥å¹¶ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # è·å–å½“å‰è®°å¿†ä½¿ç”¨æƒ…å†µ
        current_tokens = self._estimate_token_count()
        
        if current_tokens > self.max_token_limit * self.summary_trigger_ratio:
            print(f"ğŸ§  å†…å­˜ä¼˜åŒ–ï¼šå½“å‰ä½¿ç”¨{current_tokens}tokensï¼Œè§¦å‘æ™ºèƒ½æ‘˜è¦...")
            self._trigger_advanced_summary()

    def _detect_user_patterns(self, user_input: str, love_brain_level: str):
        """æ£€æµ‹ç”¨æˆ·æ‹çˆ±è¡Œä¸ºæ¨¡å¼"""
        patterns = self.long_term_memory["user_patterns"]
        
        # å…³é”®è¯æ£€æµ‹
        pattern_keywords = {
            "é‡‘é’±ä¾èµ–": ["è½¬è´¦", "å€Ÿé’±", "æŠ•èµ„", "ä¹°å•", "èŠ±é’±"],
            "æƒ…ç»ªä¾èµ–": ["æƒ³å¿µ", "ç„¦è™‘", "å¤±çœ ", "å¿ƒæƒ…", "æƒ…ç»ª"],
            "ç¤¾äº¤éš”ç¦»": ["æœ‹å‹", "å®¶äºº", "åŒäº‹", "ç¤¾äº¤", "è”ç³»"],
            "æ—¶é—´æ²‰è¿·": ["æ•´å¤©", "ä¸€ç›´", "24å°æ—¶", "ä¸åœ", "æ€»æ˜¯"]
        }
        
        for pattern_type, keywords in pattern_keywords.items():
            if any(keyword in user_input for keyword in keywords):
                if pattern_type not in patterns:
                    patterns[pattern_type] = 0
                patterns[pattern_type] += 1

    def _estimate_token_count(self) -> int:
        """ä¼°ç®—å½“å‰å†…å­˜ä½¿ç”¨çš„tokenæ•°é‡"""
        try:
            # è·å–å½“å‰ç¼“å†²åŒºå†…å®¹
            buffer_messages = self.memory.chat_memory.messages
            total_chars = sum(len(str(msg.content)) for msg in buffer_messages)
            
            # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦*1.5ï¼Œè‹±æ–‡å­—ç¬¦*0.5
            buffer_str = ' '.join(str(msg.content) for msg in buffer_messages)
            chinese_chars = len([c for c in buffer_str if '\u4e00' <= c <= '\u9fff'])
            other_chars = total_chars - chinese_chars
            return int(chinese_chars * 1.5 + other_chars * 0.5)
        except:
            return 0

    def _estimate_token_count(self) -> int:
        """ä¼°ç®—å½“å‰å†…å­˜ä½¿ç”¨çš„tokenæ•°é‡"""
        try:
            # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦*1.5ï¼Œè‹±æ–‡å•è¯*1.2
            memory_str = str(self.memory.buffer)
            chinese_chars = len([c for c in memory_str if '\u4e00' <= c <= '\u9fff'])
            other_chars = len(memory_str) - chinese_chars
            return int(chinese_chars * 1.5 + other_chars * 0.3)
        except:
            return 0

    def _trigger_advanced_summary(self):
        """è§¦å‘é«˜çº§æ‘˜è¦æœºåˆ¶"""
        # ConversationSummaryBufferMemory ä¼šè‡ªåŠ¨å¤„ç†æ‘˜è¦
        # è¿™é‡Œæˆ‘ä»¬å¯ä»¥æ·»åŠ é¢å¤–çš„ä¼˜åŒ–é€»è¾‘
        try:
            # å¼ºåˆ¶è§¦å‘æ‘˜è¦
            if hasattr(self.memory, 'prune'):
                self.memory.prune()
            
            print(f"âœ… å†…å­˜ä¼˜åŒ–å®Œæˆï¼Œå½“å‰è½®æ¬¡ï¼š{self.conversation_count}")
        except Exception as e:
            print(f"âš ï¸ å†…å­˜ä¼˜åŒ–å‡ºç°é—®é¢˜ï¼š{e}")

    def get_context_summary(self) -> str:
        """è·å–ä¸Šä¸‹æ–‡æ‘˜è¦ä¾›å·¥å…·ä½¿ç”¨"""
        summary_parts = []
        
        # æ·»åŠ å¯¹è¯è½®æ•°ä¿¡æ¯
        summary_parts.append(f"å·²å¯¹è¯{self.conversation_count}è½®")
        
        # æ·»åŠ é£é™©å†å²æ‘˜è¦
        if self.long_term_memory["risk_history"]:
            recent_risks = self.long_term_memory["risk_history"][-3:]  # æœ€è¿‘3æ¬¡
            risk_summary = "ï¼Œ".join([f"{r['level']}çº§" for r in recent_risks])
            summary_parts.append(f"é£é™©å†å²ï¼š{risk_summary}")
        
        # æ·»åŠ ç”¨æˆ·æ¨¡å¼
        if self.long_term_memory["user_patterns"]:
            top_patterns = sorted(self.long_term_memory["user_patterns"].items(), 
                                key=lambda x: x[1], reverse=True)[:2]
            pattern_summary = "ï¼Œ".join([f"{p[0]}({p[1]}æ¬¡)" for p in top_patterns])
            summary_parts.append(f"è¡Œä¸ºæ¨¡å¼ï¼š{pattern_summary}")
        
        return " | ".join(summary_parts) if summary_parts else ""
    
    def get_recent_context(self, limit: int = 3) -> List[Dict[str, str]]:
        """è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        Args:
            limit: è·å–æœ€è¿‘Nè½®å¯¹è¯
            
        Returns:
            åŒ…å«æœ€è¿‘å¯¹è¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« user_input å’Œ ai_response
        """
        recent_interactions = []
        
        try:
            # ä»memoryè·å–æœ€è¿‘çš„æ¶ˆæ¯
            if hasattr(self.memory, 'chat_memory') and hasattr(self.memory.chat_memory, 'messages'):
                messages = self.memory.chat_memory.messages
                
                # é…å¯¹ç”¨æˆ·è¾“å…¥å’ŒAIå“åº”
                for i in range(len(messages) - 1, -1, -2):  # å€’åºéå†ï¼Œæ¯æ¬¡è·³2ä¸ª
                    if i > 0 and len(recent_interactions) < limit:
                        ai_msg = messages[i] if hasattr(messages[i], 'content') else None
                        user_msg = messages[i-1] if hasattr(messages[i-1], 'content') else None
                        
                        if user_msg and ai_msg:
                            interaction = {
                                "user_input": user_msg.content,
                                "ai_response": ai_msg.content
                            }
                            recent_interactions.append(interaction)
                
                # å› ä¸ºæ˜¯å€’åºæ·»åŠ çš„ï¼Œéœ€è¦ç¿»è½¬ä»¥ä¿æŒæ—¶é—´é¡ºåº
                recent_interactions.reverse()
                
        except Exception as e:
            print(f"âš ï¸ è·å–æœ€è¿‘ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}")
        
        return recent_interactions

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        estimated_tokens = self._estimate_token_count()
        max_tokens = self.max_conversation_window * 100  # ç®€å•ä¼°ç®—
        
        return {
            "conversation_count": self.conversation_count,
            "total_interactions": self.conversation_count,  # æ·»åŠ æ€»äº¤äº’æ•°
            "short_term_count": len(self.memory.chat_memory.messages) if hasattr(self.memory, 'chat_memory') else 0,
            "long_term_count": len(self.long_term_memory["risk_history"]),
            "estimated_tokens": estimated_tokens,
            "max_tokens": max_tokens,
            "token_usage_ratio": estimated_tokens / max_tokens if max_tokens > 0 else 0,
            "risk_history_count": len(self.long_term_memory["risk_history"]),
            "pattern_count": len(self.long_term_memory["user_patterns"]),
            "user_patterns": self.long_term_memory["user_patterns"]
        }

    def clear_session(self):
        """æ¸…é™¤å½“å‰ä¼šè¯ï¼ˆä¿ç•™é•¿æœŸè®°å¿†ï¼‰"""
        self.memory.clear()
        self.conversation_count = 0
        # æ³¨æ„ï¼šä¸æ¸…é™¤long_term_memoryï¼Œä¿æŒç”¨æˆ·ç”»åƒ

    def export_memory(self) -> Dict[str, Any]:
        """å¯¼å‡ºè®°å¿†æ•°æ®ï¼ˆç”¨äºæŒä¹…åŒ–ï¼‰"""
        return {
            "conversation_count": self.conversation_count,
            "long_term_memory": self.long_term_memory,
            "memory_window": self.max_conversation_window
        }

    def import_memory(self, memory_data: Dict[str, Any]):
        """å¯¼å…¥è®°å¿†æ•°æ®ï¼ˆç”¨äºæ¢å¤ï¼‰"""
        self.conversation_count = memory_data.get("conversation_count", 0)
        self.long_term_memory = memory_data.get("long_term_memory", {
            "user_patterns": {},
            "risk_history": [],
            "key_insights": [],
            "persona_preferences": {}
        })
