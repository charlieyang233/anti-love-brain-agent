from typing import Type
import json
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from ..prompts.prompts import HELP_INNER_GUIDE, ROAST_INNER_GUIDE

class SeverityInput(BaseModel):
    user_text: str = Field(..., description="ç”¨æˆ·æœ€æ–°å‘è¨€")
    context_summary: str = Field("", description="ä¸Šæ–‡æ‘˜è¦ï¼Œå¯ä¸ºç©º")
    pre_analysis: str = Field("", description="é¢„åˆ†æç»“æœï¼ˆJSONæ ¼å¼ï¼‰")

class SeverityTool(BaseTool):
    name = "severity_analyzer"
    description = ("åæ‹çˆ±è„‘å·¥å…·ï¼šåªæœ‰ç”¨æˆ·æ˜ç¡®è®¨è®ºæ‹çˆ±ç›¸å…³è¯é¢˜æ—¶ æˆ– é¢„åˆ†æç»“æœä¸­levelä¸ºâ€œè½»/ä¸­/é‡/å±é™©â€æ—¶ï¼Œæ‰ä¼šè°ƒç”¨å½“å‰å·¥å…·æ¥ç”Ÿæˆæ¯’èˆŒé”è¯„/æƒ…æ„Ÿåˆ†æå›å¤ã€‚")
    args_schema: Type[BaseModel] = SeverityInput

    def _run(self, user_text: str, context_summary: str = "", pre_analysis: str = "") -> str:
        """æ‹çˆ±è„‘ç¨‹åº¦è¯†åˆ«å™¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è¿”å›æ ¼å¼åŒ–prompt"""
        try:
            # å¦‚æœæœ‰é¢„åˆ†æç»“æœï¼Œç›´æ¥è¿”å›æ ¼å¼åŒ–çš„ç»“æœ
            if pre_analysis:
                try:
                    # è§£æé¢„åˆ†æç»“æœ
                    analysis_data = json.loads(pre_analysis)
                    
                    # æ ¹æ®åˆ†æç»“æœç”Ÿæˆç›¸åº”çš„å›å¤
                    if analysis_data.get("switch_to_help", False):
                        # å±é™©æƒ…å†µï¼Œè¿”å›å¸®åŠ©å»ºè®®
                        return self._generate_help_response(user_text, analysis_data, context_summary)
                    else:
                        # å…¶ä»–æƒ…å†µï¼Œè¿”å›æ¯’èˆŒé”è¯„
                        return self._generate_roast_response(user_text, analysis_data, context_summary)
                        
                except json.JSONDecodeError:
                    # é¢„åˆ†æç»“æœè§£æå¤±è´¥ï¼Œç›´æ¥è¿”å›æ¯’èˆŒé”è¯„
                    pass
            
            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥è¿”å›æ¯’èˆŒé”è¯„prompt
            return self._generate_roast_response(user_text, {"level": "è½»"}, context_summary)
                    
        except Exception as e:
            # å…¨å±€é”™è¯¯å¤„ç†
            return f"""å§ğŸ§ è„‘å­å®•æœºäº†ï¼ä½†å§å»ºè®®ä½ å†·é™ä¸€ä¸‹ğŸš¬"""

    def _generate_help_response(self, user_text: str, analysis_data: dict, context_summary: str) -> str:
        """ç”Ÿæˆå¸®åŠ©å»ºè®®å›å¤"""
        from ..prompts.prompt_config import HELP_EXECUTION_PROMPT
        from ..core.config import llm
        
        prompt = HELP_EXECUTION_PROMPT.format(
            user_text=user_text,
            help_guide=HELP_INNER_GUIDE,
        )
        
        # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›å¤
        llm_instance = llm(temperature=0.7)
        response = llm_instance.invoke(prompt)
        return response.content if hasattr(response, 'content') else str(response)

    def _generate_roast_response(self, user_text: str, analysis_data: dict, context_summary: str) -> str:
        """ç”Ÿæˆæ¯’èˆŒé”è¯„å›å¤"""
        from ..prompts.prompt_config import ROAST_EXECUTION_PROMPT
        from ..core.config import llm
        
        level = analysis_data.get("level", "è½»")
        
        prompt = ROAST_EXECUTION_PROMPT.format(
            user_text=user_text,
            level=level,
            roast_guide=ROAST_INNER_GUIDE,
        )
        
        # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›å¤
        llm_instance = llm(temperature=0.7)
        response = llm_instance.invoke(prompt)
        return response.content if hasattr(response, 'content') else str(response)

    def _arun(self, *args, **kwargs):
        raise NotImplementedError
